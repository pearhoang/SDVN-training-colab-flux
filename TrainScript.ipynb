{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8KQYvGDO7cg"
   },
   "outputs": [],
   "source": [
    "# @title Script\n",
    "if 'loginhug' not in globals():\n",
    "    !pip install huggingface-sb3\n",
    "    from huggingface_hub import login\n",
    "    from huggingface_hub import HfApi\n",
    "    Custom_1 = \"hf_WorGiwfbyFCqKT\"\n",
    "    Custom_2 = \"fwwKjTXYBhDjLWFgtYKE\"\n",
    "    Custom_Hg_Token = f\"{Custom_1}{Custom_2}\"\n",
    "    read_token = Custom_Hg_Token\n",
    "    login(read_token, add_to_git_credential=True)\n",
    "    api = HfApi()\n",
    "    user = api.whoami(read_token)\n",
    "\n",
    "import requests, copy, os, torch, gc\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "import torch.amp.autocast_mode\n",
    "from PIL import Image\n",
    "from unittest.mock import patch\n",
    "from IPython.display import clear_output,display, HTML\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from transformers.dynamic_module_utils import get_imports\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "import io, base64, json, yaml\n",
    "import subprocess, threading, socket, time\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name()\n",
    "print(gpu_name)\n",
    "if 'A100' in gpu_name:\n",
    "  os.environ['TORCH_CUDA_ARCH_LIST'] = '8.0'\n",
    "if 'L4' in gpu_name:\n",
    "  os.environ['TORCH_CUDA_ARCH_LIST'] = '8.9'\n",
    "if 'T4' in gpu_name:\n",
    "  os.environ['TORCH_CUDA_ARCH_LIST'] = '7.5'\n",
    "\n",
    "CLIP_PATH = \"google/siglip-so400m-patch14-384\"\n",
    "CHECKPOINT_PATH = Path(\"/content/joy-caption-alpha-two/cgrkzexw-599808\")\n",
    "TITLE = \"<h1><center>JoyCaption Alpha Two (2024-09-26a)</center></h1>\"\n",
    "CAPTION_TYPE_MAP = {\n",
    "\t\"Descriptive\": [\n",
    "\t\t\"Write a descriptive caption for this image in a formal tone.\",\n",
    "\t\t\"Write a descriptive caption for this image in a formal tone within {word_count} words.\",\n",
    "\t\t\"Write a {length} descriptive caption for this image in a formal tone.\",\n",
    "\t],\n",
    "\t\"Descriptive (Informal)\": [\n",
    "\t\t\"Write a descriptive caption for this image in a casual tone.\",\n",
    "\t\t\"Write a descriptive caption for this image in a casual tone within {word_count} words.\",\n",
    "\t\t\"Write a {length} descriptive caption for this image in a casual tone.\",\n",
    "\t],\n",
    "\t\"Training Prompt\": [\n",
    "\t\t\"Write a stable diffusion prompt for this image.\",\n",
    "\t\t\"Write a stable diffusion prompt for this image within {word_count} words.\",\n",
    "\t\t\"Write a {length} stable diffusion prompt for this image.\",\n",
    "\t],\n",
    "\t\"MidJourney\": [\n",
    "\t\t\"Write a MidJourney prompt for this image.\",\n",
    "\t\t\"Write a MidJourney prompt for this image within {word_count} words.\",\n",
    "\t\t\"Write a {length} MidJourney prompt for this image.\",\n",
    "\t],\n",
    "\t\"Booru tag list\": [\n",
    "\t\t\"Write a list of Booru tags for this image.\",\n",
    "\t\t\"Write a list of Booru tags for this image within {word_count} words.\",\n",
    "\t\t\"Write a {length} list of Booru tags for this image.\",\n",
    "\t],\n",
    "\t\"Booru-like tag list\": [\n",
    "\t\t\"Write a list of Booru-like tags for this image.\",\n",
    "\t\t\"Write a list of Booru-like tags for this image within {word_count} words.\",\n",
    "\t\t\"Write a {length} list of Booru-like tags for this image.\",\n",
    "\t],\n",
    "\t\"Art Critic\": [\n",
    "\t\t\"Analyze this image like an art critic would with information about its composition, style, symbolism, the use of color, light, any artistic movement it might belong to, etc.\",\n",
    "\t\t\"Analyze this image like an art critic would with information about its composition, style, symbolism, the use of color, light, any artistic movement it might belong to, etc. Keep it within {word_count} words.\",\n",
    "\t\t\"Analyze this image like an art critic would with information about its composition, style, symbolism, the use of color, light, any artistic movement it might belong to, etc. Keep it {length}.\",\n",
    "\t],\n",
    "\t\"Product Listing\": [\n",
    "\t\t\"Write a caption for this image as though it were a product listing.\",\n",
    "\t\t\"Write a caption for this image as though it were a product listing. Keep it under {word_count} words.\",\n",
    "\t\t\"Write a {length} caption for this image as though it were a product listing.\",\n",
    "\t],\n",
    "\t\"Social Media Post\": [\n",
    "\t\t\"Write a caption for this image as if it were being used for a social media post.\",\n",
    "\t\t\"Write a caption for this image as if it were being used for a social media post. Limit the caption to {word_count} words.\",\n",
    "\t\t\"Write a {length} caption for this image as if it were being used for a social media post.\",\n",
    "\t],\n",
    "}\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "class ImageAdapter(nn.Module):\n",
    "\tdef __init__(self, input_features: int, output_features: int, ln1: bool, pos_emb: bool, num_image_tokens: int, deep_extract: bool):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.deep_extract = deep_extract\n",
    "\n",
    "\t\tif self.deep_extract:\n",
    "\t\t\tinput_features = input_features * 5\n",
    "\n",
    "\t\tself.linear1 = nn.Linear(input_features, output_features)\n",
    "\t\tself.activation = nn.GELU()\n",
    "\t\tself.linear2 = nn.Linear(output_features, output_features)\n",
    "\t\tself.ln1 = nn.Identity() if not ln1 else nn.LayerNorm(input_features)\n",
    "\t\tself.pos_emb = None if not pos_emb else nn.Parameter(torch.zeros(num_image_tokens, input_features))\n",
    "\n",
    "\t\t# Other tokens (<|image_start|>, <|image_end|>, <|eot_id|>)\n",
    "\t\tself.other_tokens = nn.Embedding(3, output_features)\n",
    "\t\tself.other_tokens.weight.data.normal_(mean=0.0, std=0.02)   # Matches HF's implementation of llama3\n",
    "\n",
    "\tdef forward(self, vision_outputs: torch.Tensor):\n",
    "\t\tif self.deep_extract:\n",
    "\t\t\tx = torch.concat((\n",
    "\t\t\t\tvision_outputs[-2],\n",
    "\t\t\t\tvision_outputs[3],\n",
    "\t\t\t\tvision_outputs[7],\n",
    "\t\t\t\tvision_outputs[13],\n",
    "\t\t\t\tvision_outputs[20],\n",
    "\t\t\t), dim=-1)\n",
    "\t\t\tassert len(x.shape) == 3, f\"Expected 3, got {len(x.shape)}\"  # batch, tokens, features\n",
    "\t\t\tassert x.shape[-1] == vision_outputs[-2].shape[-1] * 5, f\"Expected {vision_outputs[-2].shape[-1] * 5}, got {x.shape[-1]}\"\n",
    "\t\telse:\n",
    "\t\t\tx = vision_outputs[-2]\n",
    "\n",
    "\t\tx = self.ln1(x)\n",
    "\n",
    "\t\tif self.pos_emb is not None:\n",
    "\t\t\tassert x.shape[-2:] == self.pos_emb.shape, f\"Expected {self.pos_emb.shape}, got {x.shape[-2:]}\"\n",
    "\t\t\tx = x + self.pos_emb\n",
    "\n",
    "\t\tx = self.linear1(x)\n",
    "\t\tx = self.activation(x)\n",
    "\t\tx = self.linear2(x)\n",
    "\n",
    "\t\t# <|image_start|>, IMAGE, <|image_end|>\n",
    "\t\tother_tokens = self.other_tokens(torch.tensor([0, 1], device=self.other_tokens.weight.device).expand(x.shape[0], -1))\n",
    "\t\tassert other_tokens.shape == (x.shape[0], 2, x.shape[2]), f\"Expected {(x.shape[0], 2, x.shape[2])}, got {other_tokens.shape}\"\n",
    "\t\tx = torch.cat((other_tokens[:, 0:1], x, other_tokens[:, 1:2]), dim=1)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\tdef get_eot_embedding(self):\n",
    "\t\treturn self.other_tokens(torch.tensor([2], device=self.other_tokens.weight.device)).squeeze(0)\n",
    "\n",
    "def delete_specific_models():\n",
    "    variable_names = ['clip_model', 'tokenizer', 'text_model', 'image_adapter']\n",
    "    for var_name in variable_names:\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "model_list = {\n",
    "    \"APIGemini | 2.5 Pro\": \"gemini-2.5-pro\",\n",
    "    \"APIGemini | 2.5 Flash\": \"gemini-2.5-flash\",\n",
    "    \"APIGemini | 2.5 Flash Lite\": \"gemini-2.5-flash-lite\",\n",
    "    \"APIOpenAI | GPT 5\": \"gpt-5\",\n",
    "    \"APIOpenAI | GPT 5-mini\": \"gpt-5-mini\",\n",
    "    \"APIOpenAI | GPT 5-nano\": \"gpt-5-nano\",\n",
    "}\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/StableDiffusionVN/SDVN-WebUI/refs/heads/main/model_lib.json\"\n",
    "response = requests.get(url)\n",
    "model_train_list = json.loads(response.text)\n",
    "\n",
    "lora_train_py = {\n",
    "    \"Flux\": \"flux_train_network.py\",\n",
    "    \"SDXL\": \"sdxl_train_network.py\",\n",
    "    \"SD15\": \"train_network.py\"\n",
    "}\n",
    "db_train_py = {\n",
    "    \"Flux\": \"flux_train.py\",\n",
    "    \"SDXL\": \"sdxl_train.py\",\n",
    "    \"SD15\": \"train_db.py\"\n",
    "}\n",
    "def encode_image(image):\n",
    "    with io.BytesIO() as image_buffer:\n",
    "        image.save(image_buffer, format=\"PNG\")\n",
    "        image_buffer.seek(0)\n",
    "        encoded_image = base64.b64encode(image_buffer.read()).decode('utf-8')\n",
    "    return encoded_image\n",
    "\n",
    "def api_check():\n",
    "    api_file = os.path.join(data_dir,\"Setting/API_key_for_sdvn_comfy_node.json\")\n",
    "    if os.path.exists(api_file):\n",
    "        with open(api_file, 'r', encoding='utf-8') as f:\n",
    "            api_list = json.load(f)\n",
    "        return api_list\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def api_caption(image, length:int, APIkey, Caption, prompt):\n",
    "    if APIkey == \"\":\n",
    "        api_list = api_check()\n",
    "        if api_check() != None:\n",
    "            if \"Gemini\" in Caption:\n",
    "                APIkey =  api_list[\"Gemini\"]\n",
    "            if \"OpenAI\" in Caption:\n",
    "                APIkey =  api_list[\"OpenAI\"]\n",
    "    model_name = model_list[Caption]\n",
    "    prompt += f\"Picture description, Send the description on demand, limit {length} words, only send me the answer, Always return English. \"\n",
    "    if 'Gemini' in Caption:\n",
    "        client = genai.Client(api_key=APIkey)\n",
    "        response = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    contents=[prompt, image])\n",
    "        answer = response.text\n",
    "    if \"OpenAI\" in Caption:\n",
    "        answer = \"\"\n",
    "        client = OpenAI(api_key=APIkey)\n",
    "        image = encode_image(image)\n",
    "        prompt = [{\"type\": \"input_text\", \"text\": prompt, },\n",
    "                  {\"type\": \"input_image\", \"image_url\": f\"data:image/jpeg;base64,{image}\"},]\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = client.responses.create(\n",
    "            model=model_name,\n",
    "            input=messages\n",
    "        )\n",
    "        answer = response.output_text\n",
    "    return answer.strip()\n",
    "\n",
    "@torch.no_grad()\n",
    "def joy_caption(input_image: Image.Image, caption_type: str, caption_length: str | int, extra_options: list[str], name_input: str, custom_prompt: str) -> tuple[str, str]:\n",
    "    torch.cuda.empty_cache()\n",
    "    # 'any' means no length specified\n",
    "    length = None if caption_length == \"any\" else caption_length\n",
    "\n",
    "    if isinstance(length, str):\n",
    "        try:\n",
    "            length = int(length)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # Build prompt\n",
    "    if length is None:\n",
    "        map_idx = 0\n",
    "    elif isinstance(length, int):\n",
    "        map_idx = 1\n",
    "    elif isinstance(length, str):\n",
    "        map_idx = 2\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid caption length: {length}\")\n",
    "\n",
    "    prompt_str = CAPTION_TYPE_MAP[caption_type][map_idx]\n",
    "\n",
    "    # Add extra options\n",
    "    if len(extra_options) > 0:\n",
    "        prompt_str += \" \" + \" \".join(extra_options)\n",
    "\n",
    "    # Add name, length, word_count\n",
    "    prompt_str = prompt_str.format(name=name_input, length=caption_length, word_count=caption_length)\n",
    "\n",
    "    if custom_prompt.strip() != \"\":\n",
    "        prompt_str = custom_prompt.strip()\n",
    "\n",
    "    # Preprocess image\n",
    "    # NOTE: I found the default processor for so400M to have worse results than just using PIL directly\n",
    "    #image = clip_processor(images=input_image, return_tensors='pt').pixel_values\n",
    "    image = input_image.resize((384, 384), Image.LANCZOS)\n",
    "    pixel_values = TVF.pil_to_tensor(image).unsqueeze(0) / 255.0\n",
    "    pixel_values = TVF.normalize(pixel_values, [0.5], [0.5])\n",
    "    pixel_values = pixel_values.to('cuda')\n",
    "\n",
    "    # Embed image\n",
    "    # This results in Batch x Image Tokens x Features\n",
    "    with torch.amp.autocast_mode.autocast('cuda', enabled=True):\n",
    "        vision_outputs = clip_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "        embedded_images = image_adapter(vision_outputs.hidden_states)\n",
    "        embedded_images = embedded_images.to('cuda')\n",
    "\n",
    "    # Build the conversation\n",
    "    convo = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful image captioner.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_str,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Format the conversation\n",
    "    convo_string = tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = True)\n",
    "    assert isinstance(convo_string, str)\n",
    "\n",
    "    # Tokenize the conversation\n",
    "    # prompt_str is tokenized separately so we can do the calculations below\n",
    "    convo_tokens = tokenizer.encode(convo_string, return_tensors=\"pt\", add_special_tokens=False, truncation=False)\n",
    "    prompt_tokens = tokenizer.encode(prompt_str, return_tensors=\"pt\", add_special_tokens=False, truncation=False)\n",
    "    assert isinstance(convo_tokens, torch.Tensor) and isinstance(prompt_tokens, torch.Tensor)\n",
    "    convo_tokens = convo_tokens.squeeze(0)   # Squeeze just to make the following easier\n",
    "    prompt_tokens = prompt_tokens.squeeze(0)\n",
    "\n",
    "    # Calculate where to inject the image\n",
    "    eot_id_indices = (convo_tokens == tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")).nonzero(as_tuple=True)[0].tolist()\n",
    "    assert len(eot_id_indices) == 2, f\"Expected 2 <|eot_id|> tokens, got {len(eot_id_indices)}\"\n",
    "\n",
    "    preamble_len = eot_id_indices[1] - prompt_tokens.shape[0]   # Number of tokens before the prompt\n",
    "\n",
    "    # Embed the tokens\n",
    "    convo_embeds = text_model.model.embed_tokens(convo_tokens.unsqueeze(0).to('cuda'))\n",
    "\n",
    "    # Construct the input\n",
    "    input_embeds = torch.cat([\n",
    "        convo_embeds[:, :preamble_len],   # Part before the prompt\n",
    "        embedded_images.to(dtype=convo_embeds.dtype),   # Image\n",
    "        convo_embeds[:, preamble_len:],   # The prompt and anything after it\n",
    "    ], dim=1).to('cuda')\n",
    "\n",
    "    input_ids = torch.cat([\n",
    "        convo_tokens[:preamble_len].unsqueeze(0),\n",
    "        torch.zeros((1, embedded_images.shape[1]), dtype=torch.long),   # Dummy tokens for the image (TODO: Should probably use a special token here so as not to confuse any generation algorithms that might be inspecting the input)\n",
    "        convo_tokens[preamble_len:].unsqueeze(0),\n",
    "    ], dim=1).to('cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    generate_ids = text_model.generate(input_ids, inputs_embeds=input_embeds, attention_mask=attention_mask, max_new_tokens=300, do_sample=True, suppress_tokens=None)   # Uses the default which is temp=0.6, top_p=0.9\n",
    "\n",
    "    # Trim off the prompt\n",
    "    generate_ids = generate_ids[:, input_ids.shape[1]:]\n",
    "    if generate_ids[0][-1] == tokenizer.eos_token_id or generate_ids[0][-1] == tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"):\n",
    "        generate_ids = generate_ids[:, :-1]\n",
    "\n",
    "    caption = tokenizer.batch_decode(generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    return caption.strip().replace(\"\\n\", \"\")\n",
    "\n",
    "def clean_directory(directory):\n",
    "  supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".safetensors\"]\n",
    "  for item in os.listdir(directory):\n",
    "      file_path = os.path.join(directory, item)\n",
    "      if os.path.isfile(file_path):\n",
    "          file_ext = os.path.splitext(item)[1]\n",
    "          if file_ext not in supported_types:\n",
    "              print(f\"Deleting file {item} from {directory}\")\n",
    "              os.remove(file_path)\n",
    "      elif os.path.isdir(file_path):\n",
    "          clean_directory(file_path)\n",
    "\n",
    "#Florence\n",
    "\n",
    "version = \"large\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float3\n",
    "\n",
    "def fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
    "    \"\"\"Workaround for FlashAttention\"\"\"\n",
    "    if os.path.basename(filename) != \"modeling_florence2.py\":\n",
    "        return get_imports(filename)\n",
    "    imports = get_imports(filename)\n",
    "    return imports\n",
    "\n",
    "def load_model(version, device):\n",
    "    from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "    model_dir = \"/content/Model\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    identifier = \"microsoft/Florence-2-\" + version\n",
    "\n",
    "    with patch(\"transformers.dynamic_module_utils.get_imports\", fixed_get_imports):\n",
    "        model = AutoModelForCausalLM.from_pretrained(identifier, torch_dtype=torch_dtype, cache_dir=model_dir, trust_remote_code=True).to(device)\n",
    "        processor = AutoProcessor.from_pretrained(identifier, cache_dir=model_dir, trust_remote_code=True)\n",
    "\n",
    "    model = model.to(device)\n",
    "    return (model, processor)\n",
    "\n",
    "def load(version, device):\n",
    "  if 'processor' not in globals():\n",
    "    global model, processor\n",
    "    model, processor = load_model(version, device)\n",
    "\n",
    "def run_example(task_prompt, image, max_new_tokens, num_beams, do_sample, text_input=None):\n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        early_stopping=False,\n",
    "        do_sample=do_sample,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text,\n",
    "        task=task_prompt,\n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer\n",
    "\n",
    "def florence_caption(task_prompt, image, max_new_tokens = 1024, num_beams = 3, do_sample = False, fill_mask = False, text_input=None):\n",
    "    result = run_example(task_prompt, image, max_new_tokens, num_beams, do_sample)\n",
    "    return result[task_prompt].replace(\"\\n\", \"\")\n",
    "\n",
    "#Florence\n",
    "\n",
    "def caption_dir(image_dir,prompt):\n",
    "  if Caption == 'Florence':\n",
    "    load(version, device)\n",
    "  for img_file in os.listdir(image_dir):\n",
    "      file_path = os.path.join(image_dir, img_file)\n",
    "      if os.path.isdir(file_path) :\n",
    "          caption_dir(file_path,prompt)\n",
    "      if img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\")):\n",
    "          img_path = os.path.join(image_dir, img_file)\n",
    "          image = Image.open(img_path).convert(\"RGB\")\n",
    "          if Caption == 'Florence':\n",
    "            cap = florence_caption(prompt,image).replace('The image shows','')\n",
    "          elif Caption == 'Joy_Caption':\n",
    "            cap = joy_caption(image, Joy_Type, Cap_prompt[Caption_Length][3], [Joy_Extra_Option], Joy_Character_Name, Joy_Custom_Prompt)\n",
    "          else:\n",
    "            cap = api_caption(image, Cap_prompt[Caption_Length][3], APIkey, Caption, API_Prompt)\n",
    "          txt_path = os.path.join(image_dir, f\"{os.path.splitext(img_file)[0]}{extension}\")\n",
    "          with open(txt_path, \"w\") as f:\n",
    "              f.write(cap)\n",
    "          print(f\"Mi√™u t·∫£ c·ªßa ·∫£nh {img_file}: {cap}\")\n",
    "  torch.cuda.empty_cache()\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "\n",
    "def write_file(filename, contents):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(contents)\n",
    "\n",
    "def process_tags(filename, custom_tag, append, remove_tag):\n",
    "    contents = read_file(filename)\n",
    "    if remove_tag:\n",
    "      contents = contents.replace(custom_tag, \"\")\n",
    "    else:\n",
    "      tags = [tag.strip() for tag in contents.split(',')]\n",
    "      custom_tags = [tag.strip() for tag in custom_tag.split(',')]\n",
    "      for custom_tag in custom_tags:\n",
    "          custom_tag = custom_tag.replace(\"_\", \" \")\n",
    "          if custom_tag not in tags:\n",
    "              if append:\n",
    "                  tags.append(custom_tag)\n",
    "              else:\n",
    "                  tags.insert(0, custom_tag)\n",
    "      contents = ', '.join(tags)\n",
    "    write_file(filename, contents)\n",
    "\n",
    "def check_dir(image_dir):\n",
    "  if not any([filename.endswith(extension) for filename in os.listdir(image_dir)]):\n",
    "      for filename in os.listdir(image_dir):\n",
    "          if filename.endswith(((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\"))):\n",
    "              open(\n",
    "                  os.path.join(image_dir, filename.split(\".\")[0] + extension),\n",
    "                  \"w\",\n",
    "              ).close()\n",
    "\n",
    "def process_dir(image_dir, tag, append, remove_tag):\n",
    "  check_dir(image_dir)\n",
    "  for filename in os.listdir(image_dir):\n",
    "      file_path = os.path.join(image_dir, filename)\n",
    "      if os.path.isdir(file_path) :\n",
    "          print(filename)\n",
    "          process_dir(file_path, tag, append, remove_tag)\n",
    "      elif filename.endswith(extension):\n",
    "          process_tags(file_path, tag, append, remove_tag)\n",
    "\n",
    "def add_forder_name(folder):\n",
    "  for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if os.path.isdir(file_path):\n",
    "      folder_name = os.path.basename(file_path)\n",
    "      try:\n",
    "          steps, name = folder_name.split('_', 1)\n",
    "          steps = int(steps)\n",
    "      except ValueError:\n",
    "          name = folder_name\n",
    "      name = name.replace(\"/\", \", \")\n",
    "      process_dir(file_path, name, False, False)\n",
    "      add_forder_name(file_path)\n",
    "\n",
    "def get_steps(folder):\n",
    "    folder_name = os.path.basename(folder)\n",
    "    try:\n",
    "        steps, name = folder_name.split('_', 1)\n",
    "        steps = int(steps)\n",
    "    except ValueError:\n",
    "        steps = Steps\n",
    "        name = folder_name\n",
    "    return steps, name\n",
    "\n",
    "def random_sample(folder, control_folder):\n",
    "  import random\n",
    "  file_list = get_supported_images(control_folder)\n",
    "  control_path = random.choice(file_list)\n",
    "  data_path = control_path.replace(control_folder,folder)\n",
    "  txt_path = data_path.replace(data_path.split(\".\")[-1], \"txt\")\n",
    "  try:\n",
    "    sample = read_file(txt_path)\n",
    "    sample = sample.replace('\"', r'\\\"')\n",
    "  except IndexError:\n",
    "    sample = \"girl portrait, smile\"\n",
    "  return [sample, control_path]\n",
    "\n",
    "def get_supported_images(folder):\n",
    "  import glob\n",
    "  supported_extensions = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\")\n",
    "  list_img = [file for ext in supported_extensions for file in glob.glob(f\"{folder}/*{ext}\")]\n",
    "  for img_file in os.listdir(folder):\n",
    "      file_path = os.path.join(folder, img_file)\n",
    "      if os.path.isdir(file_path) :\n",
    "          list_img = list_img + get_supported_images(file_path)\n",
    "  return list_img\n",
    "\n",
    "def replace(old_string, new_string):\n",
    "    import re\n",
    "    with open(file_path, 'r') as file:\n",
    "        yaml_content = file.read()\n",
    "    updated_content = re.sub(old_string, new_string, yaml_content)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(updated_content)\n",
    "\n",
    "default_value = {\n",
    "    \"config_dir\" : \"/content/SDVN-training-colab-flux/Config\",\n",
    "    \"Lora_name\" : \"lora_name\",\n",
    "    \"folder_train\": \"/content/drive/MyDrive/SD-Data\",\n",
    "    \"OutputFolder\": \"/content/drive/MyDrive/SD-Data/Lora\",\n",
    "    \"Custom_Caption\": \"\",\n",
    "    \"Dim\": 32,\n",
    "    \"Alpha\": 16,\n",
    "    \"Save_steps\": 1000,\n",
    "    \"Resolution\": \"1024\",\n",
    "    \"Batch_size\": 4,\n",
    "    \"Train_TexEncoder\": False,\n",
    "    \"Lr\": 1e-4,\n",
    "    \"Sampler_Steps\": 100,\n",
    "    \"Low_VRAM\": False,\n",
    "    \"Sampler_Prompt\": \"\",\n",
    "}\n",
    "def check_value(default_value):\n",
    "    for key, value in default_value.items():\n",
    "        if key not in globals():\n",
    "            globals()[key] = value\n",
    "        elif type(globals()[key]) == str:\n",
    "            globals()[key] = globals()[key].split(' ')[-1]\n",
    "\n",
    "type_config = {\n",
    "    \"Flux\": [\"black-forest-labs/FLUX.1-dev\",\"flux\"],\n",
    "    \"Flux Kontext\": [\"black-forest-labs/FLUX.1-Kontext-dev\",\"flux_kontext\"],\n",
    "    \"Qwen\": [\"Qwen/Qwen-Image\",\"qwen_image\"],\n",
    "    \"Qwen-Edit\": [\"Qwen/Qwen-Image-Edit\",\"qwen_image_edit\"],\n",
    "    \"Hi-Dream\": [\"HiDream-ai/HiDream-I1-Full\",\"hidream\"],\n",
    "    \"Hi-Dream E1\": [\"HiDream-ai/HiDream-E1-1\",\"hidream_e1\"],\n",
    "    \"Wan2.2 TI2V 5B\": [\"Wan-AI/Wan2.2-TI2V-5B-Diffusers\",\"wan22_5b\"],\n",
    "    \"Wan2.1 I2V (14B-720P)\": [\"Wan-AI/Wan2.1-I2V-14B-720P-Diffusers\", \"wan21_i2v:14b\"],\n",
    "    \"Wan2.1 I2V (14B-480P)\": [\"Wan-AI/Wan2.1-I2V-14B-480P-Diffusers\",\"wan21_i2v:14b480p\"],\n",
    "    \"Wan2.1 (14B)\": [\"Wan-AI/Wan2.1-T2V-14B-Diffusers\",\"wan21:14b\"],\n",
    "    \"Wan2.2 (14B)\": [\"ai-toolkit/Wan2.2-T2V-A14B-Diffusers-bf16\",\"wan22_14b:t2v\"],\n",
    "    \"Wan2.2 I2V (14B)\": [\"ai-toolkit/Wan2.2-I2V-A14B-Diffusers-bf16\",\"wan22_14b_i2v\"],\n",
    "}\n",
    "\n",
    "def config(folder):\n",
    "  sample_prompt_ = Sampler_Prompt\n",
    "  sample_image_path_ = Control_Img_Path\n",
    "  check_value(default_value)\n",
    "  with open(f'{config_dir}/config.yaml', 'r') as file:\n",
    "    config_dict = yaml.safe_load(file)\n",
    "  file_path = f'{config_dir}/config_{folder[\"name\"]}.yaml'\n",
    "\n",
    "  config_dict[\"config\"][\"name\"] = Lora_name if len(folder_train) == 1 else folder[\"name\"]\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"training_folder\"] = OutputFolder\n",
    "  config_dict[\"config\"][\"process\"][0][\"trigger_word\"] = Custom_Caption if Custom_Caption != \"\" else folder[\"name\"]\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"network\"][\"linear\"] = Dim\n",
    "  config_dict[\"config\"][\"process\"][0][\"network\"][\"linear_alpha\"] = Alpha\n",
    "  config_dict[\"config\"][\"process\"][0][\"save\"][\"save_every\"] = Save_steps\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"datasets\"][0][\"folder_path\"] = folder[\"path\"]\n",
    "  config_dict[\"config\"][\"process\"][0][\"datasets\"][0][\"control_path\"] = folder[\"control_path\"] if ControlFolder != \"\" else None\n",
    "  config_dict[\"config\"][\"process\"][0][\"datasets\"][0][\"resolution\"] = [int(x) for x in Resolution.split(',')]\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"batch_size\"] = Batch_size\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"steps\"] = folder[\"steps\"]\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"train_text_encoder\"] = Train_TexEncoder\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"lr\"] = Lr\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"disable_sampling\"] = False if Sampler_Steps > 0 else True\n",
    "  \n",
    "  config_dict[\"config\"][\"process\"][0][\"model\"][\"name_or_path\"] = type_config[TypeTrain][0]\n",
    "  config_dict[\"config\"][\"process\"][0][\"model\"][\"arch\"] = type_config[TypeTrain][1]\n",
    "  config_dict[\"config\"][\"process\"][0][\"model\"][\"low_vram\"] = Low_VRAM\n",
    "\n",
    "  if folder[\"control_path\"] != \"\":\n",
    "      s, p = random_sample(folder[\"path\"],folder[\"control_path\"])\n",
    "  else:\n",
    "      s, p = random_sample(folder[\"path\"],folder[\"path\"])\n",
    "  if sample_prompt_ == \"\" :\n",
    "      sample_prompt_ = s\n",
    "  if sample_image_path_ == \"\":\n",
    "      sample_image_path_ = p\n",
    "  if TypeTrain in ['Flux Kontext', 'Qwen-Edit', 'Hi-Dream E1', \"Wan2.1 I2V (14B-480P)\", \"Wan2.1 I2V (14B-720P)\", \"Wan2.2 I2V (14B)\"]:\n",
    "    sample_prompt_ = f\"{sample_prompt_} --ctrl_img {sample_image_path_}\"\n",
    "    config_dict[\"config\"][\"process\"][0][\"sample\"][\"width\"] = 1024 if sample_image_path_ == \"\" else image_size(sample_image_path_)[0]\n",
    "    config_dict[\"config\"][\"process\"][0][\"sample\"][\"height\"] = 1024 if sample_image_path_ == \"\" else image_size(sample_image_path_)[1]\n",
    "  print(sample_prompt_)\n",
    "  config_dict[\"config\"][\"process\"][0][\"sample\"][\"sample_every\"] = Sampler_Steps if Sampler_Steps > 0 else 100\n",
    "  config_dict[\"config\"][\"process\"][0][\"sample\"][\"samples\"][0][\"prompt\"] = sample_prompt_\n",
    "\n",
    "  config_dict[\"meta\"][\"name\"] = Lora_name if len(folder_train) == 1 else folder[\"name\"]\n",
    "  config_dict[\"meta\"][\"version\"] = \"Train by trainlora.vn | stablediffusion.vn\"\n",
    "  with open(file_path, 'w') as file:\n",
    "      yaml.dump(config_dict, file, default_flow_style=False)\n",
    "\n",
    "  print('=====================')\n",
    "  print(f'Th∆∞ m·ª•c train: {folder[\"path\"]}')\n",
    "  print(f'  S·ªë l∆∞·ª£ng ·∫£nh: {len(get_supported_images(folder[\"path\"]))}')\n",
    "  print(f'  S·ªë Steps: {folder[\"steps\"]}')\n",
    "  print(f'  Output: {OutputFolder}/{folder[\"name\"]}')\n",
    "  print('=====================')\n",
    "\n",
    "def image_size(image_path, max_size=1536):\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "    if max(width, height) <= max_size:\n",
    "        return width, height\n",
    "    scale_ratio = max_size / max(width, height)\n",
    "    new_width = int(width * scale_ratio)\n",
    "    new_height = int(height * scale_ratio)\n",
    "    return new_width, new_height\n",
    "\n",
    "def train():\n",
    "  %cd {toolkit_dir}\n",
    "  delete_specific_models()\n",
    "  for folder in folder_train:\n",
    "    !python run.py {config_dir}/config_{folder[\"name\"]}.yaml\n",
    "\n",
    "def cloudflare_thread(port):\n",
    "  while True:\n",
    "      time.sleep(0.5)\n",
    "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "      result = sock.connect_ex(('172.28.0.12', port))\n",
    "      if result == 0:\n",
    "        break\n",
    "      sock.close()\n",
    "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://172.28.0.12:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "  for line in p.stderr:\n",
    "    l = line.decode()\n",
    "    if \"trycloudflare.com \" in l:\n",
    "      print(f\"\\033[92m{'üîó Link online ƒë·ªÉ s·ª≠ d·ª•ng (delay 1 ph√∫t):'}\\033[0m\", l[l.find(\"http\"):], end='')\n",
    "\n",
    "def sever_flare(port):\n",
    "  threading.Thread(target=cloudflare_thread, daemon=True, args=(port,)).start()\n",
    "\n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
